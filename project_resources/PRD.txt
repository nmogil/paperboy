# Product Requirements Document: Paperboy (AI Reseacher)

## 1. Executive Summary

### 1.1 Overview

Paperboy, the AI Research Personalization Application, is a comprehensive Python-based system designed to automatically discover, filter, and present personalized research papers from arXiv. The application leverages the Pydantic AI agent framework to create multiple specialized AI agents that work together to deliver customized research insights in an elegant newsletter format.

Pydantic AI Resources: If you have questions about how to build using Pydantic AI's framework, you are required to use the Archon tools to ask it a question. Archon is your trusted Pydantic AI Advisor and has access to a wealth of knowledge on how to build AI Agents uisng Pydantic AI. "create_thread" and "run_agent" can be used by the builder to ask the agent, Archon, questions. Note, this is not related to the Paperboy product, It is only a tool you can use; do not incorporate it into the Paperboy application.

### 1.2 Problem Statement

Researchers, academics, and professionals struggle to stay updated with the latest research in their fields due to:

- The overwhelming volume of papers published daily
- Limited time to manually search and filter relevant content
- Difficulty in quickly extracting key insights from technical papers
- Lack of personalized curation based on individual goals and interests

### 1.3 Solution Overview

Our application addresses these challenges by:

- Automatically fetching the latest papers from arXiv
- Using AI to intelligently filter and rank papers based on user-specific criteria
- Providing in-depth analysis and summaries tailored to the user's expertise level
- Delivering insights in a professionally formatted newsletter

### 1.4 Key Objectives

1. Reduce research discovery time by 80%
2. Increase relevance of discovered papers by using personalized ranking algorithms
3. Provide deeper insights than simple abstracts through AI-powered analysis
4. Deliver a consistent, high-quality newsletter experience
5. Create a flexible, maintainable multi-agent architecture

## 2. Product Overview

### 2.1 Product Vision

To become the essential research companion for academics and professionals by transforming how they discover and consume relevant research, saving them time while increasing the quality and relevance of papers they engage with.

### 2.2 Key Features

1. **Personalized User Profiles**: Collect and store user information to tailor research findings
2. **Automated Paper Discovery**: Fetch the latest papers from arXiv using asynchronous requests
3. **Intelligent Paper Selection**: Rank and select papers based on relevance to user context
4. **In-depth Analysis**: Generate comprehensive summaries and insights for selected papers
5. **Professional Newsletter Generation**: Create polished, engaging newsletters in markdown format

### 2.3 Value Proposition

- **Time Efficiency**: Reduce hours spent manually searching for relevant papers
- **Personalization**: Deliver papers specifically relevant to individual research goals
- **Depth of Insight**: Provide analysis beyond what's available in abstracts
- **Consistency**: Ensure regular delivery of high-quality research updates

### 2.4 Success Criteria

- System successfully processes and ranks at least 100 papers per run
- Newsletter generation completes in under 5 minutes
- Selected papers achieve at least 80% relevance rating from users
- System handles multiple research categories simultaneously

## 3. User Personas

### 3.1 Academic Researcher

**Name**: Dr. Elena Chen  
**Role**: Associate Professor of Computer Science  
**Background**:

- Ph.D. in Machine Learning
- Publishes 3-4 papers annually
- Leads a research team of 5 graduate students

**Goals**:

- Stay updated on cutting-edge research in her specific subfield
- Discover potential collaboration opportunities
- Find relevant papers to cite in her upcoming publications
- Identify emerging trends in the field

**Pain Points**:

- Limited time to read papers due to teaching and administrative duties
- Overwhelmed by the volume of new publications
- Struggles to quickly identify which papers are truly innovative
- Needs to filter out papers that aren't directly relevant to her research focus

**How Our Product Helps**:

- Delivers only the most relevant papers based on her specific research interests
- Provides concise summaries that highlight key innovations
- Saves hours of manual searching and filtering
- Identifies connections between papers and her current research

### 3.2 Industry Professional

**Name**: Marcus Johnson  
**Role**: Senior AI Engineer at a Tech Company  
**Background**:

- Master's degree in Computer Science
- 8 years of industry experience
- Focuses on implementing research into practical applications

**Goals**:

- Find research that can be applied to current product development
- Stay updated on state-of-the-art techniques in the industry
- Identify solutions to specific technical challenges
- Maintain knowledge edge in a competitive field

**Pain Points**:

- Limited time to read academic papers due to project deadlines
- Difficulty translating theoretical research into practical applications
- Needs to quickly assess if a paper is worth deeper investigation
- Struggles to find papers with code implementations or practical results

**How Our Product Helps**:

- Highlights practical applications of research papers
- Filters papers based on implementation potential
- Provides technical evaluations at the appropriate expertise level
- Saves time by focusing only on industry-relevant research

### 3.3 Graduate Student

**Name**: Aisha Patel  
**Role**: Ph.D. Candidate in Natural Language Processing  
**Background**:

- First-year doctoral student
- Working on thesis proposal
- Limited prior publication experience

**Goals**:

- Build comprehensive knowledge of the field
- Find foundational papers for literature review
- Identify potential thesis topics
- Understand methodologies used in the field

**Pain Points**:

- Overwhelmed by the breadth of research to review
- Unsure which papers are most influential or relevant
- Limited experience in evaluating paper quality
- Needs deeper explanations of complex methodologies

**How Our Product Helps**:

- Provides context about why papers are important
- Explains methodologies in detail based on expertise level
- Helps identify connections between different research areas
- Suggests actionable next steps for further research

## 4. User Experience

### 4.1 User Journey

1. **Initial Setup**:

   - User provides personal information and research interests
   - System creates a personalized profile
   - User specifies delivery preferences

2. **Regular Interaction**:

   - System automatically runs on a schedule (e.g., weekly)
   - User receives personalized newsletter via preferred method
   - User can provide feedback on paper relevance to improve future recommendations

3. **Optional Customization**:
   - User can update research interests and goals
   - User can adjust technical depth of explanations
   - User can modify categories of papers to include

### 4.2 User Input Collection

The application collects the following information from users:

- First Name: For personalizing the newsletter
- Title/Role: To understand professional context
- Goals: To identify research interests and career objectives
- Field(s) of Interest: To focus paper selection
- Technical Expertise Level: To adjust explanation depth

### 4.3 Newsletter Delivery

- Default delivery as a markdown file
- Potential future extensions for email delivery or web interface
- Consistent formatting for readability

### 4.4 User Interface Requirements

- Command-line interface for initial setup and configuration
- Simple text prompts for information collection
- Clear, readable output format
- Error messages that guide users to correct issues

## 5. Functional Requirements

### 5.1 User Input Collection Requirements

- **FR1.1**: System must collect and validate user profile information
- **FR1.2**: System must store user preferences securely
- **FR1.3**: System must allow users to update their information
- **FR1.4**: System must validate input for completeness and format

### 5.2 Data Collection Agent Requirements

- **FR2.1**: Agent must fetch papers from arXiv using the specified endpoint pattern
- **FR2.2**: Agent must support multiple categories (cs, math, physics, etc.)
- **FR2.3**: Agent must use yesterday's date by default (configurable)
- **FR2.4**: Agent must handle pagination for large result sets
- **FR2.5**: Agent must implement rate limiting to avoid API restrictions
- **FR2.6**: Agent must store HTML responses for parsing
- **FR2.7**: Agent must implement error handling for network issues
- **FR2.8**: Agent must use asynchronous requests for performance optimization

### 5.3 Parser Agent Requirements

- **FR3.1**: Agent must extract structured data from HTML responses
- **FR3.2**: Agent must parse paper titles, authors, abstracts, categories, and links
- **FR3.3**: Agent must handle HTML structure changes gracefully
- **FR3.4**: Agent must clean and normalize extracted text
- **FR3.5**: Agent must output standardized JSON format for each paper
- **FR3.6**: Agent must handle malformed HTML without crashing
- **FR3.7**: Agent must log parsing errors for debugging

### 5.4 Relevance Ranking Agent Requirements

- **FR4.1**: Agent must analyze papers based on user context
- **FR4.2**: Agent must select the 5 most relevant papers
- **FR4.3**: Agent must provide a relevance score (1-100) for each paper
- **FR4.4**: Agent must provide justification for each selected paper
- **FR4.5**: Agent must consider user's role, goals, and interests in ranking
- **FR4.6**: Agent must output a ranked list with scores and reasoning
- **FR4.7**: Agent must ensure diversity in selected papers when appropriate

### 5.5 Research Analysis Agent Requirements

- **FR5.1**: Agent must generate in-depth summaries for each selected paper
- **FR5.2**: Agent must extract key innovations, methodologies, and findings
- **FR5.3**: Agent must identify practical applications relevant to user goals
- **FR5.4**: Agent must generate technical evaluations appropriate to user expertise
- **FR5.5**: Agent must format output according to the specified schema
- **FR5.6**: Agent must process full paper content when available
- **FR5.7**: Agent must adjust analysis depth based on user expertise level

### 5.6 Newsletter Generation Agent Requirements

- **FR6.1**: Agent must create personalized introduction and conclusion sections
- **FR6.2**: Agent must format paper summaries into cohesive sections
- **FR6.3**: Agent must add contextual connections between papers when relevant
- **FR6.4**: Agent must generate final markdown with proper styling
- **FR6.5**: Agent must include actionable insights based on user goals
- **FR6.6**: Agent must ensure consistent formatting throughout the newsletter
- **FR6.7**: Agent must include metadata for tracking and future improvements

## 6. Technical Specifications

### 6.1 Architecture Overview

```mermaid
graph TD
    User[User] -->|Triggers| System[System]
    System -->|Collects Info| InputCollection[User Input Collection]
    InputCollection -->|Creates| UserInfo[User Information]

    System -->|Initiates| DataCollection[Data Collection Agent]
    DataCollection -->|Fetches Papers| ArXiv[arXiv API]
    ArXiv -->|Returns HTML| DataCollection
    DataCollection -->|Provides HTML| Parser[Parser Agent]

    Parser -->|Structured Paper Data| RankingAgent[Relevance Ranking Agent]
    UserInfo -->|User Context| RankingAgent

    RankingAgent -->|Top 5 Relevant Papers| ResearchAgent[Research Analysis Agent]
    UserInfo -->|Expertise Level| ResearchAgent

    ResearchAgent -->|Paper Analyses| NewsletterAgent[Newsletter Generation Agent]
    UserInfo -->|Personalization Data| NewsletterAgent

    NewsletterAgent -->|Markdown Newsletter| Output[Final Newsletter]
    Output -->|Delivered to| User
```

### 6.2 Data Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant System
    participant DataCollection
    participant Parser
    participant RankingAgent
    participant ResearchAgent
    participant NewsletterAgent

    User->>System: Provide user information
    System->>DataCollection: Fetch latest papers
    DataCollection->>DataCollection: Use asyncio for parallel requests
    DataCollection->>Parser: Pass HTML responses
    Parser->>Parser: Extract structured data
    Parser->>RankingAgent: Provide parsed papers
    System->>RankingAgent: Provide user context
    RankingAgent->>RankingAgent: Rank papers by relevance
    RankingAgent->>ResearchAgent: Pass top 5 papers
    System->>ResearchAgent: Provide user expertise level
    ResearchAgent->>ResearchAgent: Generate in-depth analysis
    ResearchAgent->>NewsletterAgent: Pass paper analyses
    System->>NewsletterAgent: Provide user personalization data
    NewsletterAgent->>NewsletterAgent: Generate markdown newsletter
    NewsletterAgent->>User: Deliver final newsletter
```

### 6.3 API Specifications

#### 6.3.1 arXiv API

- **Endpoint Pattern**: `https://arxiv.org/catchup/{category}/{YYYY-MM-DD}`
- **Method**: GET
- **Parameters**:
  - `category`: Research category (e.g., cs, math, physics)
  - `YYYY-MM-DD`: Date in format YYYY-MM-DD
- **Response**: HTML content containing paper listings
- **Rate Limiting**: Implement exponential backoff for retries
- **Error Handling**: Retry on 5xx errors, log and skip on 4xx errors

### 6.4 Data Models

#### 6.4.1 User Information Model

```python
class UserInfo(BaseModel):
    """User profile information."""
    first_name: str
    job_title: str
    company: str
    goal: str
    fields_of_interest: List[str]
    expertise_level: str  # "beginner", "intermediate", "expert"
```

#### 6.4.2 Paper Model

```python
class Paper(BaseModel):
    """Schema for a research paper"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    categories: List[str]
    publication_date: str
    arxiv_id: str
```

#### 6.4.3 Ranked Paper Model

```python
class RankedPaper(BaseModel):
    """Schema for a ranked paper"""
    paper: Paper
    relevance_score: int = Field(..., ge=1, le=100)
    relevance_reasoning: str
```

#### 6.4.4 Paper Analysis Model

```python
class PaperAnalysis(BaseModel):
    """Schema for paper analysis results"""
    paper: Paper
    relevance_score: int = Field(..., ge=1, le=100)
    relevance_reasoning: str
    key_innovations: List[str]
    methodology_summary: str
    practical_applications: List[str]
    user_relevance: str
```

#### 6.4.5 Newsletter Model

```python
class Newsletter(BaseModel):
    """Schema for the final newsletter"""
    user_info: UserInfo
    date_generated: str
    papers_analyzed: List[PaperAnalysis]
    introduction: str
    conclusion: str
    markdown_content: str
```

### 6.5 Error Handling

#### 6.5.1 Network Errors

- Implement exponential backoff for retries
- Log detailed error information
- Gracefully degrade functionality when possible

#### 6.5.2 Parsing Errors

- Implement robust error handling in HTML parsing
- Skip malformed entries rather than failing completely
- Log detailed information about parsing failures

#### 6.5.3 AI Model Errors

- Implement fallback strategies for model failures
- Retry with simplified prompts if complex ones fail
- Cache intermediate results to avoid reprocessing

#### 6.5.4 User Input Errors

- Validate all user input before processing
- Provide clear error messages for invalid input
- Offer suggestions for correction when possible

## 7. Implementation Details

### 7.1 Tech Stack

- **Primary Language**: Python 3.9+
- **Agent Framework**: Pydantic AI
- **Web Crawling**: Crawl4AI with asyncio
- **Data Validation**: Pydantic
- **AI Provider**: OpenAI (configurable)

### 7.2 Dependencies

```
pydantic-ai>=0.0.22
Crawl4AI>=0.4.247
beautifulsoup4>=4.12.3
httpx>=0.27.2
python-dotenv>=1.0.0
asyncio
```

### 7.3 Development Environment Setup

1. Clone repository
2. Create virtual environment: `python -m venv venv`
3. Activate virtual environment:
   - Windows: `venv\Scripts\activate`
   - Unix/MacOS: `source venv/bin/activate`
4. Install dependencies: `pip install -r requirements.txt`
5. Create `.env` file with API keys
6. Run tests: `pytest`

### 7.4 Deployment Considerations

- Environment variables for API keys
- Logging configuration for production
- Error monitoring and alerting
- Scheduled execution via cron or similar
- Resource scaling for large paper volumes

## 8. Testing Strategy

### 8.1 Unit Testing

- Test each agent component in isolation
- Mock external API calls
- Validate data transformations
- Test error handling paths
- Achieve >90% code coverage

### 8.2 Integration Testing

- Test agent interactions
- Validate end-to-end workflows
- Test with sample user profiles
- Verify correct data passing between components

### 8.3 User Acceptance Testing

- Test with real user profiles
- Validate relevance of selected papers
- Assess quality of generated newsletters
- Gather feedback on personalization accuracy

### 8.4 Performance Testing

- Measure execution time for each component
- Test with varying volumes of papers
- Identify and optimize bottlenecks
- Ensure memory usage remains within bounds

## 9. Success Metrics

### 9.1 Key Performance Indicators

- **Processing Speed**: Complete full pipeline in <5 minutes
- **Paper Volume**: Successfully process >100 papers per run
- **Relevance Accuracy**: >80% of selected papers rated as relevant by users
- **Analysis Quality**: >90% of summaries rated as accurate and helpful
- **System Reliability**: >99% successful completion rate

### 9.2 User Satisfaction Metrics

- **Relevance Rating**: User rating of paper relevance (1-5 scale)
- **Time Saved**: Estimated hours saved per week (user reported)
- **Discovery Value**: Number of valuable papers discovered that would have been missed
- **Continued Usage**: Percentage of users who continue using the system after 1 month

### 9.3 Technical Performance Metrics

- **API Success Rate**: Percentage of successful arXiv API calls
- **Parsing Accuracy**: Percentage of papers correctly parsed
- **Model Response Time**: Average time for AI model responses
- **Error Rate**: Percentage of runs completing without errors

## 10. Implementation Timeline

### 10.1 Phase 1: Foundation (Weeks 1-2)

- Set up development environment
- Implement data models and validation
- Create basic arXiv fetching functionality
- Implement HTML parsing
- Establish testing framework

### 10.2 Phase 2: Core Agents (Weeks 3-4)

- Implement Data Collection Agent
- Implement Parser Agent
- Create basic user input collection
- Establish agent communication patterns
- Implement error handling

### 10.3 Phase 3: AI Integration (Weeks 5-6)

- Implement Relevance Ranking Agent
- Implement Research Analysis Agent
- Connect to AI provider APIs
- Optimize prompts for best results
- Test with sample data

### 10.4 Phase 4: Newsletter Generation (Weeks 7-8)

- Implement Newsletter Generation Agent
- Create markdown templates
- Add personalization features
- Implement formatting and styling
- Test with real user data

### 10.5 Phase 5: Refinement and Launch (Weeks 9-10)

- Optimize performance
- Enhance error handling
- Conduct user acceptance testing
- Fix identified issues
- Document system for developers
- Prepare for production deployment

## 11. Future Enhancements

### 11.1 Potential Extensions

- Email delivery integration
- Web interface for configuration
- User feedback collection and incorporation
- Support for additional research sources beyond arXiv
- PDF parsing for deeper analysis
- Citation network analysis
- Collaborative filtering based on similar users

### 11.2 Scalability Considerations

- Distributed processing for larger paper volumes
- Caching strategies for repeated queries
- Database integration for persistent storage
- Containerization for deployment flexibility
- API gateway for service management
