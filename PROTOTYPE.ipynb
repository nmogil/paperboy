{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payper Boy Protoype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetching Latest articles using crawl4ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\"https://arxiv.org/catchup/cs/2025-04-08\")\n",
    "        print(result.markdown[:3000])  # Print first 300 chars\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extracting article details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, JsonCssExtractionStrategy\n",
    "import datetime\n",
    "import re\n",
    "import nest_asyncio\n",
    "import traceback\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Schema 1: Extract metadata from <dd> ---\n",
    "schema_dd = {\n",
    "    \"name\": \"ArXiv Article Metadata\",\n",
    "    \"baseSelector\": \"dl#articles > dd\", # Target the <dd> element directly\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"raw_title\",\n",
    "            \"selector\": \"div.list-title\", # Selector relative to <dd>\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"authors\",\n",
    "            \"selector\": \"div.list-authors a\", # Selector relative to <dd>\n",
    "            \"type\": \"list\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"author_name\", \"type\": \"text\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"raw_subjects\",\n",
    "            \"selector\": \"div.list-subjects\", # Selector relative to <dd>\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"primary_subject\",\n",
    "            \"selector\": \"div.list-subjects span.primary-subject\", # Selector relative to <dd>\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"raw_comments\",\n",
    "            \"selector\": \"div.list-comments\", # Selector relative to <dd>\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"raw_journal_ref\",\n",
    "            \"selector\": \"div.list-journal-ref\", # Selector relative to <dd>\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Schema 2: Extract IDs and Links from <dt> ---\n",
    "schema_dt = {\n",
    "    \"name\": \"ArXiv Article Links\",\n",
    "    \"baseSelector\": \"dl#articles > dt\", # Target the <dt> element\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"arxiv_id_from_href\",\n",
    "            \"selector\": \"a[title='Abstract']\",\n",
    "            \"type\": \"attribute\",\n",
    "            \"attribute\": \"href\",\n",
    "            \"regex\": r\"/abs/([^/]+)\",  # More permissive regex to catch various ID formats\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"arxiv_id_from_text\",\n",
    "            \"selector\": \"a[title='Abstract']\",\n",
    "            \"type\": \"text\",\n",
    "            \"regex\": r\"arXiv:([^\\s]+)\",  # More permissive regex\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"abstract_url_rel\",\n",
    "            \"selector\": \"a[title='Abstract']\",\n",
    "            \"type\": \"attribute\",\n",
    "            \"attribute\": \"href\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"pdf_url_rel\",\n",
    "            \"selector\": \"a[title='Download PDF']\",\n",
    "            \"type\": \"attribute\",\n",
    "            \"attribute\": \"href\",\n",
    "            \"default\": None\n",
    "        },\n",
    "         {\n",
    "            \"name\": \"html_url\",\n",
    "            \"selector\": \"a[title='View HTML']\",\n",
    "            \"type\": \"attribute\",\n",
    "            \"attribute\": \"href\",\n",
    "            \"default\": None\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Schema 3: Try to detect section headers (optional enhancement) ---\n",
    "schema_headers = {\n",
    "    \"name\": \"ArXiv Section Headers\",\n",
    "    \"baseSelector\": \"dl#articles > dt.newheader\", \n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"header_text\",\n",
    "            \"selector\": \"\",  # Element itself\n",
    "            \"type\": \"text\",\n",
    "            \"default\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"position\",\n",
    "            \"selector\": \"\",\n",
    "            \"type\": \"attribute\",\n",
    "            \"attribute\": \"data-position\",  # We'll add this attribute during processing\n",
    "            \"default\": None\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Helper Function for Post-Processing (Improved) ---\n",
    "def process_and_merge_articles(dt_data_list, dd_data_list, base_url=\"https://arxiv.org\"):\n",
    "    \"\"\"Merges dt and dd data and cleans fields with improved ID extraction.\"\"\"\n",
    "    merged_articles = []\n",
    "    num_items = min(len(dt_data_list), len(dd_data_list))\n",
    "    print(f\"Attempting to merge {num_items} dt/dd pairs.\")\n",
    "    \n",
    "    id_extraction_failures = 0\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        dt_data = dt_data_list[i]\n",
    "        dd_data = dd_data_list[i]\n",
    "        processed = {}\n",
    "\n",
    "        # --- Improved ArXiv ID Handling ---\n",
    "        id_from_href_raw = dt_data.get('arxiv_id_from_href')\n",
    "        id_from_text_raw = dt_data.get('arxiv_id_from_text')\n",
    "        canonical_id = None\n",
    "        \n",
    "        # Try to extract the ID using various methods\n",
    "        for id_source in [id_from_href_raw, id_from_text_raw]:\n",
    "            if not id_source:\n",
    "                continue\n",
    "                \n",
    "            # Try standard format YYMM.NNNNN or YYMM.NNNNNvN\n",
    "            std_match = re.search(r\"(\\d{4}\\.\\d{5}(?:v\\d+)?)\", id_source)\n",
    "            if std_match:\n",
    "                canonical_id = std_match.group(1)\n",
    "                break\n",
    "                \n",
    "            # Try alternate format for older papers\n",
    "            alt_match = re.search(r\"([a-z-]+(?:\\.[A-Z]{2})?\\/\\d{7}(?:v\\d+)?)\", id_source)\n",
    "            if alt_match:\n",
    "                canonical_id = alt_match.group(1)\n",
    "                break\n",
    "                \n",
    "            # Last resort: try to extract any sequence that might be an ID\n",
    "            last_resort_match = re.search(r\"([^\\/\\s]+)(?:v\\d+)?$\", id_source)\n",
    "            if last_resort_match:\n",
    "                canonical_id = last_resort_match.group(1)\n",
    "                break\n",
    "\n",
    "        if not canonical_id:\n",
    "            id_extraction_failures += 1\n",
    "            context_hint = dd_data.get('raw_title', 'N/A')[:50] or dt_data.get('abstract_url_rel', 'N/A') or f'Entry {i+1}'\n",
    "            print(f\"Warning: Skipping entry {i+1}, failed to extract valid canonical arXiv ID. Context hint: {context_hint}\")\n",
    "            print(f\"  - href source: {id_from_href_raw}\")\n",
    "            print(f\"  - text source: {id_from_text_raw}\")\n",
    "            continue\n",
    "\n",
    "        processed['arxiv_id'] = f\"arXiv:{canonical_id}\"\n",
    "\n",
    "        # --- URL Construction (from dt_data) ---\n",
    "        abstract_rel = dt_data.get('abstract_url_rel')\n",
    "        processed['abstract_url'] = f\"{base_url}{abstract_rel}\" if abstract_rel else f\"{base_url}/abs/{canonical_id}\"\n",
    "\n",
    "        pdf_rel = dt_data.get('pdf_url_rel')\n",
    "        if pdf_rel:\n",
    "            processed['pdf_url'] = f\"{base_url}{pdf_rel}\"\n",
    "\n",
    "        html_url = dt_data.get('html_url')\n",
    "        if html_url:\n",
    "            if isinstance(html_url, str) and not html_url.startswith(('http://', 'https://')):\n",
    "                if not html_url.startswith('/'):\n",
    "                    html_url = '/' + html_url\n",
    "                processed['html_url'] = f\"{base_url}{html_url}\"\n",
    "            else:\n",
    "                processed['html_url'] = html_url\n",
    "\n",
    "        # --- Text Cleaning (from dd_data) ---\n",
    "        raw_title = dd_data.get('raw_title', '')\n",
    "        processed['title'] = re.sub(r'^Title:\\s*', '', raw_title, flags=re.IGNORECASE).strip() if raw_title else None\n",
    "\n",
    "        raw_subjects = dd_data.get('raw_subjects', '')\n",
    "        processed['subjects'] = re.sub(r'^Subjects:\\s*', '', raw_subjects, flags=re.IGNORECASE).strip() if raw_subjects else None\n",
    "\n",
    "        raw_comments = dd_data.get('raw_comments', '')\n",
    "        processed['comments'] = re.sub(r'^Comments:\\s*', '', raw_comments, flags=re.IGNORECASE).strip() if raw_comments else None\n",
    "\n",
    "        raw_journal_ref = dd_data.get('raw_journal_ref', '')\n",
    "        processed['journal_ref'] = re.sub(r'^Journal-ref:\\s*', '', raw_journal_ref, flags=re.IGNORECASE).strip() if raw_journal_ref else None\n",
    "\n",
    "        processed['primary_subject'] = dd_data.get('primary_subject')\n",
    "\n",
    "        # --- Author List (from dd_data) ---\n",
    "        author_list = dd_data.get('authors', [])\n",
    "        processed['authors'] = [auth.get('author_name') for auth in author_list if auth.get('author_name')]\n",
    "\n",
    "        # --- Ensure essential keys exist ---\n",
    "        processed.setdefault('title', None)\n",
    "        processed.setdefault('subjects', None)\n",
    "        processed.setdefault('primary_subject', None)\n",
    "        processed.setdefault('comments', None)\n",
    "        processed.setdefault('journal_ref', None)\n",
    "        processed.setdefault('authors', [])\n",
    "        processed.setdefault('pdf_url', None)\n",
    "        processed.setdefault('html_url', None)\n",
    "\n",
    "        merged_articles.append(processed)\n",
    "    \n",
    "    # Print summary of failures\n",
    "    if id_extraction_failures:\n",
    "        print(f\"Total ID extraction failures: {id_extraction_failures} out of {num_items} ({id_extraction_failures/num_items*100:.1f}%)\")\n",
    "    \n",
    "    return merged_articles\n",
    "\n",
    "# --- Asynchronous Function to Crawl and Extract (Modified) ---\n",
    "async def get_arxiv_new_submissions(target_date: str, collect_all=True):\n",
    "    \"\"\"\n",
    "    Fetches and extracts submissions from arXiv CS catchup for a specific date.\n",
    "    \n",
    "    Args:\n",
    "        target_date: Date in YYYY-MM-DD format\n",
    "        collect_all: If True, collects all articles without trying to separate new vs replacements\n",
    "    \"\"\"\n",
    "    base_url = \"https://arxiv.org\"\n",
    "    try:\n",
    "        date_obj = datetime.datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "        url_date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        target_url = f\"https://arxiv.org/catchup/cs/{url_date_str}\"\n",
    "        print(f\"Targeting URL: {target_url}\")\n",
    "    except ValueError:\n",
    "        print(\"Error: Invalid date format. Please use YYYY-MM-DD.\")\n",
    "        return []\n",
    "\n",
    "    # --- Setup Extraction Strategies ---\n",
    "    strategy_dd = JsonCssExtractionStrategy(schema_dd, verbose=False)\n",
    "    strategy_dt = JsonCssExtractionStrategy(schema_dt, verbose=False)\n",
    "\n",
    "    # --- Setup Crawler Configurations ---\n",
    "    config_dd = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, extraction_strategy=strategy_dd)\n",
    "    config_dt = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, extraction_strategy=strategy_dt)\n",
    "\n",
    "    # --- Run Crawlers ---\n",
    "    dd_data_list = []\n",
    "    dt_data_list = []\n",
    "\n",
    "    try:\n",
    "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
    "            print(\"Running crawler for <dd> elements...\")\n",
    "            result_dd = await crawler.arun(url=target_url, config=config_dd)\n",
    "            if result_dd.success and result_dd.extracted_content:\n",
    "                try:\n",
    "                    dd_data_list = json.loads(result_dd.extracted_content)\n",
    "                    print(f\"Successfully extracted {len(dd_data_list)} <dd> entries.\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse <dd> JSON. First 200 chars: {result_dd.extracted_content[:200]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"Crawl for <dd> failed or no content. Error: {result_dd.error_message}\")\n",
    "                return [] # Stop if we can't get <dd> data\n",
    "\n",
    "            print(\"Running crawler for <dt> elements...\")\n",
    "            result_dt = await crawler.arun(url=target_url, config=config_dt)\n",
    "            if result_dt.success and result_dt.extracted_content:\n",
    "                try:\n",
    "                    dt_data_list = json.loads(result_dt.extracted_content)\n",
    "                    print(f\"Successfully extracted {len(dt_data_list)} <dt> entries.\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse <dt> JSON. First 200 chars: {result_dt.extracted_content[:200]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"Crawl for <dt> failed or no content. Error: {result_dt.error_message}\")\n",
    "                return []\n",
    "\n",
    "        # --- Merge and Post-process ---\n",
    "        if len(dt_data_list) != len(dd_data_list):\n",
    "            print(f\"Warning: Mismatch between number of <dt> ({len(dt_data_list)}) and <dd> ({len(dd_data_list)}) elements. Merging based on minimum.\")\n",
    "\n",
    "        all_processed_articles = process_and_merge_articles(dt_data_list, dd_data_list, base_url)\n",
    "        \n",
    "        if not all_processed_articles:\n",
    "            print(\"Warning: No articles were successfully merged and processed.\")\n",
    "            return []\n",
    "            \n",
    "        # Instead of filtering for \"new submissions\" vs \"replacements\", return all articles \n",
    "        # This avoids the premature stopping issue\n",
    "        print(f\"Collected {len(all_processed_articles)} total articles.\")\n",
    "        return all_processed_articles\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        # Add more context if possible\n",
    "        if 'result_dd' in locals() and hasattr(result_dd, 'extracted_content'):\n",
    "            print(f\"DD content hint: {result_dd.extracted_content[:500] if result_dd.extracted_content else 'None'}\")\n",
    "        if 'result_dt' in locals() and hasattr(result_dt, 'extracted_content'):\n",
    "            print(f\"DT content hint: {result_dt.extracted_content[:500] if result_dt.extracted_content else 'None'}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# --- Post-processing Function to Categorize Articles ---\n",
    "def categorize_articles(articles):\n",
    "    \"\"\"\n",
    "    This function attempts to categorize articles into new submissions and replacements.\n",
    "    It uses a heuristic based on journal references and sequences.\n",
    "    \"\"\"\n",
    "    new_submissions = []\n",
    "    cross_listings = []\n",
    "    replacements = []\n",
    "    \n",
    "    # Simple heuristic: articles with journal references are likely replacements\n",
    "    # You might need to refine this based on actual ArXiv structure\n",
    "    for article in articles:\n",
    "        journal_ref = article.get('journal_ref')\n",
    "        # Advanced logic could go here\n",
    "        # For now, just assume all are new submissions\n",
    "        new_submissions.append(article)\n",
    "    \n",
    "    return {\n",
    "        'new_submissions': new_submissions,\n",
    "        'cross_listings': cross_listings,\n",
    "        'replacements': replacements\n",
    "    }\n",
    "\n",
    "# --- Example Usage ---\n",
    "async def main():\n",
    "    target_date = \"2025-04-01\" # Date from sample HTML\n",
    "    print(f\"Attempting to fetch CS submissions announced on {target_date}...\")\n",
    "\n",
    "    # Get all submissions without filtering\n",
    "    all_submissions = await get_arxiv_new_submissions(target_date, collect_all=True)\n",
    "\n",
    "    if all_submissions:\n",
    "        print(f\"\\n--- Extracted {len(all_submissions)} Total Submissions ---\")\n",
    "        \n",
    "        # Optional: Categorize articles (if you implement this)\n",
    "        # categorized = categorize_articles(all_submissions)\n",
    "        # print(f\"Categorized: {len(categorized['new_submissions'])} new, \"\n",
    "        #      f\"{len(categorized['cross_listings'])} cross-listed, \"\n",
    "        #      f\"{len(categorized['replacements'])} replacements\")\n",
    "        \n",
    "        print(\"\\nFirst 3 entries:\")\n",
    "        print(json.dumps(all_submissions[:3], indent=2, ensure_ascii=False))\n",
    "        \n",
    "        if len(all_submissions) > 3:\n",
    "            print(\"\\nLast 3 entries:\")\n",
    "            print(json.dumps(all_submissions[-3:], indent=2, ensure_ascii=False))\n",
    "\n",
    "        output_filename = f\"arxiv_cs_submissions_{target_date}.json\"\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_submissions, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nFull data saved to {output_filename}\")\n",
    "        \n",
    "        # If you want to save categorized data\n",
    "        # for category, data in categorized.items():\n",
    "        #     if data:\n",
    "        #         cat_filename = f\"arxiv_cs_{category}_{target_date}.json\"\n",
    "        #         with open(cat_filename, 'w', encoding='utf-8') as f:\n",
    "        #             json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        #         print(f\"Saved {len(data)} {category} to {cat_filename}\")\n",
    "    else:\n",
    "        print(\"\\nNo submissions data was extracted.\")\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "else:\n",
    "    # For Jupyter notebook environment\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Find Relevent Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions from agent.py\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the current directory to the path so we can import agent.py\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the rank_articles function from agent.py\n",
    "from agent import rank_articles\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define user information for article ranking\n",
    "user_info = {\n",
    "    \"name\": \"Dr. Jane Smith\",  # Replace with your name\n",
    "    \"title\": \"Computer Science Professor\",  # Replace with your title\n",
    "    \"goals\": \"I'm researching new approaches to natural language processing and looking for papers on transformer architectures and their applications.\"  # Replace with your research interests\n",
    "}\n",
    "\n",
    "# Function to run the article ranking\n",
    "async def run_article_ranking(articles, top_n=5):\n",
    "    \"\"\"Rank articles based on user information.\"\"\"\n",
    "    print(f\"Ranking {len(articles)} articles for relevance to user's interests...\")\n",
    "    \n",
    "    # Call the rank_articles function from agent.py\n",
    "    articles_ranked = await rank_articles(user_info, articles, top_n=top_n)\n",
    "    \n",
    "    if not articles_ranked:\n",
    "        print(\"Failed to rank articles\")\n",
    "        return\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTop {len(articles_ranked)} Relevant Papers:\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, article in enumerate(articles_ranked, 1):\n",
    "        print(f\"\\n{i}. {article.title}\")\n",
    "        print(f\"   Authors: {', '.join(article.authors)}\")\n",
    "        print(f\"   Subject: {article.subject}\")\n",
    "        print(f\"   Relevance Score: {article.relevance_score}/100\")\n",
    "        print(f\"   Reasoning: {article.reasoning}\")\n",
    "        print(f\"   URL: {article.abstract_url}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return articles_ranked\n",
    "\n",
    "# Example usage with the articles from the previous cell\n",
    "# This assumes you have already run the get_arxiv_new_submissions function\n",
    "# and have the articles in a variable called 'all_submissions'\n",
    "\n",
    "# Uncomment and modify the following code to run the ranking:\n",
    "target_date = \"2025-04-01\"\n",
    "all_submissions = await get_arxiv_new_submissions(target_date, collect_all=True)\n",
    "if all_submissions:\n",
    "    ranked_articles = await run_article_ranking(all_submissions, top_n=5)\n",
    "else:\n",
    "    print(\"No submissions data was extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Summarize Every Article"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
